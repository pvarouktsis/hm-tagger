{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from plotnine import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = pd.read_csv('./100mentors_finallized.csv')\n",
    "hmf = hm\\\n",
    "    .groupby(by='text')['tag']\\\n",
    "    .apply(lambda tags: frozenset(tags))\\\n",
    "    .reset_index(name='tags')\n",
    "hmf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk = pd.read_csv('./wikipedia_finallized.csv')\n",
    "wkf = wk\\\n",
    "    .groupby(by='text')['tag']\\\n",
    "    .apply(lambda tags: frozenset(tags))\\\n",
    "    .reset_index(name='tags')\n",
    "wkf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "HM_CLASSES = frozenset(itertools.chain.from_iterable(hmf['tags']))\n",
    "WK_CLASSES = frozenset(itertools.chain.from_iterable(wkf['tags']))\n",
    "\n",
    "CLASSES = frozenset(HM_CLASSES | WK_CLASSES)\n",
    "NOT_HM_CLASSES = frozenset(HM_CLASSES ^ CLASSES)\n",
    "NOT_WK_CLASSES = frozenset(WK_CLASSES ^ CLASSES)\n",
    "\n",
    "print(f'*** All Classes ({len(CLASSES)}) ***')\n",
    "print()\n",
    "print('\\n'.join(sorted(CLASSES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('popular')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# https://wordnet.princeton.edu/documentation/wndb5wn\n",
    "def get_wn_pos(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return 'n' # noun\n",
    "    elif tag.startswith('VB'):\n",
    "        return 'v' # verb\n",
    "    elif tag.startswith('JJ'):\n",
    "        return 'a' # adjective\n",
    "    elif tag.startswith('RB'):\n",
    "        return 'r' # adverb\n",
    "    else:\n",
    "        return 'n' # noun => default of lemmatize()\n",
    "\n",
    "def validate(token):\n",
    "    if len(token) > 1\\\n",
    "        and token.isalpha()\\\n",
    "        and token not in STOPWORDS:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8') # remove non-english chars\n",
    "    text = re.sub(r'#[A-Za-z0-9_]+', '', text)            # remove hashtags\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    lemmatized = []\n",
    "    for token, tag in tagged:\n",
    "        if not validate(token):\n",
    "            continue\n",
    "\n",
    "        wn_tag = get_wn_pos(tag)\n",
    "        lemmatized.append( wnl.lemmatize(token, wn_tag) )\n",
    "    \n",
    "    text = ' '.join(token for token in lemmatized)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = hmf['text'].values[0]\n",
    "print('Before')\n",
    "print('=' * (len('Before') + 1))\n",
    "print(ex)\n",
    "print()\n",
    "print('After')\n",
    "print('=' * (len('After') + 1))\n",
    "print(preprocess(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model training with \"scikit-learn\" & \"scikit-multilearn\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from joblib import load, dump\n",
    "from time import time\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "# from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# https://github.com/scikit-learn-contrib/imbalanced-learn/issues/340\n",
    "# from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.ensemble import RakelD, RakelO\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers\n",
    "transformers = [\n",
    "    {\n",
    "        'id': 'cv',\n",
    "        'name': 'Count Vectorizer',\n",
    "        'model': CountVectorizer(),\n",
    "        'parameters': {\n",
    "            'cv__preprocessor': [preprocess],\n",
    "            'cv__max_df': [.6],\n",
    "            'cv__min_df': [2],\n",
    "            'cv__binary': [False],\n",
    "        },\n",
    "        'parameter_grid': [\n",
    "            {\n",
    "                'cv__preprocessor': [preprocess],\n",
    "                'cv__max_df': [.5, .6, .7],\n",
    "                'cv__min_df': [1, 2, 5],\n",
    "                'cv__binary': [False],\n",
    "            },\n",
    "            {\n",
    "                'cv__preprocessor': [preprocess],\n",
    "                'cv__max_df': [.5, .6, .7],\n",
    "                'cv__min_df': [1],\n",
    "                'cv__binary': [True],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'id': 'tfidf',\n",
    "        'name': 'TF-IDF Transformer',\n",
    "        'model': TfidfTransformer(),\n",
    "        'parameters': {\n",
    "            'tfidf__norm': ['l2'],\n",
    "            'tfidf__use_idf': [True],\n",
    "            'tfidf__smooth_idf': [True],\n",
    "        },\n",
    "        'parameter_grid': [\n",
    "            {\n",
    "                'tfidf__norm': ['l2'],\n",
    "                'tfidf__use_idf': [True],\n",
    "                'tfidf__smooth_idf': [True],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approaches\n",
    "approaches = [\n",
    "    {\n",
    "        # Problem Transformers\n",
    "        'id': 'pt',\n",
    "        'name': 'Problem Transformers',\n",
    "        'models': [\n",
    "            {\n",
    "                'id': 'br',\n",
    "                'name': 'Binary Relevance',\n",
    "                'model': BinaryRelevance(),\n",
    "                'parameters': {},\n",
    "                'parameter_grid': [{}],\n",
    "            },\n",
    "            {\n",
    "                'id': 'cc',\n",
    "                'name': 'Classifier Chain',\n",
    "                'model': ClassifierChain(),\n",
    "                'parameters': {},\n",
    "                'parameter_grid': [{}],\n",
    "            },\n",
    "        ],\n",
    "        'prefix': '',\n",
    "    },\n",
    "    {\n",
    "        # Ensembles\n",
    "        'id': 'ens',\n",
    "        'name': 'Ensembles',\n",
    "        'models': [\n",
    "            {\n",
    "                'id': 'rakeld',\n",
    "                'name': 'RakelD',\n",
    "                'model': RakelD(),\n",
    "                'parameters': {\n",
    "                    'rakeld__labelset_size': [3]\n",
    "                },\n",
    "                'parameter_grid': [\n",
    "                    {\n",
    "                        'rakeld__labelset_size': [3],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                'id': 'rakelo',\n",
    "                'name': 'RakelO',\n",
    "                'model': RakelO(),\n",
    "                'parameters': {\n",
    "                    'rakelo__labelset_size': [3]\n",
    "                },\n",
    "                'parameter_grid': [\n",
    "                    {\n",
    "                        'rakelo__labelset_size': [3],\n",
    "                    },\n",
    "                ],\n",
    "            },   \n",
    "        ],\n",
    "        'prefix': 'base_',\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "base_classifiers = [\n",
    "    {\n",
    "        'id': 'sgd',\n",
    "        'name': 'Stochastic Gradient Descent Classifier',\n",
    "        'model': SGDClassifier(),\n",
    "        'parameters': {},\n",
    "        'parameter_grid': [\n",
    "            {\n",
    "                'classifier__loss': [\"hinge\", \"squared_hinge\", \"modified_huber\", \"log\"],\n",
    "                'classifier__penalty': [\"l2\"],\n",
    "                'classifier__alpha': [1e-4],\n",
    "            },\n",
    "        ],\n",
    "        'supports_parallel': True,\n",
    "    },\n",
    "    {\n",
    "        'id': 'svm',\n",
    "        'name': 'Support Vector Machine Classifier',\n",
    "        'model': SVC(),\n",
    "        'parameters': {\n",
    "            'classifier__C': [1],\n",
    "            'classifier__kernel': ['rbf'],\n",
    "            'classifier__tol': [1e-3],\n",
    "            'classifier__gamma': ['scale'],\n",
    "        },\n",
    "        'parameter_grid': [\n",
    "            {\n",
    "                'classifier__C': [1, 1.5, 2],\n",
    "                'classifier__kernel': ['rbf', 'sigmoid'],\n",
    "                'classifier__tol': [1e-4, 1e-3],\n",
    "                'classifier__gamma': ['scale', 2],\n",
    "            },\n",
    "        ],\n",
    "        'supports_parallel': False,\n",
    "    },\n",
    "    {\n",
    "        'id': 'mnb',\n",
    "        'name': 'Multinomial Naive Bayes Classifier',\n",
    "        'model': MultinomialNB(),\n",
    "        'parameters': {},\n",
    "        'parameter_grid': [\n",
    "            {\n",
    "                'classifier__alpha': [1e-2, 1e-1, 1],\n",
    "                'classifier__fit_prior': [False, True],\n",
    "            },\n",
    "        ],\n",
    "        'supports_parallel': True,\n",
    "    },\n",
    "    {\n",
    "        'id': 'knn',\n",
    "        'name': 'K Nearest Neighbors Classifier',\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'parameters': {\n",
    "            'classifier__n_neighbors': [5],\n",
    "            'classifier__weights': ['uniform'],\n",
    "            'classifier__algorithm': ['auto'],\n",
    "            'classifier__p': [2],\n",
    "        },\n",
    "        'parameter_grid': [\n",
    "            {\n",
    "                'classifier__n_neighbors': [1, 5],\n",
    "                'classifier__weights': ['uniform', 'distance'],\n",
    "                'classifier__algorithm': ['auto'],\n",
    "                'classifier__p': [2],\n",
    "            },\n",
    "        ],\n",
    "        'supports_parallel': True,\n",
    "    },\n",
    "    {\n",
    "        'id': 'rf',\n",
    "        'name': 'Random Forest Classifier',\n",
    "        'model': RandomForestClassifier(),\n",
    "        'parameters': {\n",
    "            'classifier__n_estimators': [100],\n",
    "            'classifier__criterion': ['gini'],\n",
    "            'classifier__max_depth': [None],\n",
    "            'classifier__min_samples_split': [10],\n",
    "            'classifier__max_features': ['auto'],\n",
    "        },\n",
    "        'parameter_grid': [\n",
    "            {\n",
    "                'classifier__n_estimators': [100, 400],\n",
    "                'classifier__criterion': ['gini', 'entropy'],\n",
    "                'classifier__max_depth': [None, 10],\n",
    "                'classifier__min_samples_split': [2, 4],\n",
    "                'classifier__max_features': ['auto'],\n",
    "            },\n",
    "        ],\n",
    "        'supports_parallel': True,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def execute(ds, ex=list(), pgs=True, cv=3, error=np.nan, verbose=0, path=''):\n",
    "    \"\"\"\n",
    "    ds [dict]: containing the keys `X_train`, `X_test`, `y_train`, `y_test`\n",
    "        and their values, datasets\n",
    "    opg [list(dict)]: override default parameters\n",
    "    pgs [bool]: GridSearch with param_grid\n",
    "    error ['raise', num]: GridSearch's error\n",
    "    verbose [int]: GridSearch's verbose\n",
    "    \n",
    "    TODOs:\n",
    "    - cached transformers in grid-search?, cross-validation problem?\n",
    "    \"\"\"\n",
    "    # approaches\n",
    "    for approach in approaches:\n",
    "        \n",
    "        # approaches' classifiers\n",
    "        for classifier in approach['models']:\n",
    "            print(f\"{approach['name']} - {classifier['name']}\")\n",
    "            print(\"=\" * (len(approach['name']) + 3 + len(classifier['name'])))\n",
    "            \n",
    "            # pipeline\n",
    "            pipeline = Pipeline(steps=[\n",
    "                (transformers[0]['id'], transformers[0]['model']),\n",
    "                (transformers[1]['id'], transformers[1]['model']),\n",
    "                (classifier['id'], classifier['model'])\n",
    "            ])\n",
    "            \n",
    "            # extra parameters\n",
    "            extra_parameters = {}\n",
    "            if classifier['id'] == 'rakelo':\n",
    "                extra_parameters['rakelo__model_count'] = [ds['y_train'].shape[1] * 2]\n",
    "\n",
    "            # base classifiers\n",
    "            for base_classifier in base_classifiers:\n",
    "                if base_classifier['id'] in ex:\n",
    "                    continue\n",
    "                \n",
    "                # hide warnings\n",
    "                with warnings.catch_warnings(record=True) as ws:\n",
    "                    \n",
    "                    # parameter grid\n",
    "                    parameter_grid = []\n",
    "                    if pgs:\n",
    "                        # base classifier's parameter grids\n",
    "                        base_classifier_parameter_grids = []\n",
    "                        for base_classifier_parameter_grid in base_classifier['parameter_grid']:\n",
    "                            base_classifier_parameter_grids.append({ \n",
    "                                f\"{classifier['id']}__{approach['prefix']}{key}\": \n",
    "                                    value for key, value in base_classifier_parameter_grid.items()\n",
    "                            })\n",
    "\n",
    "                        # parameter grig combinations\n",
    "                        parameter_grid_combinations = list(itertools.product(\n",
    "                            # transformers\n",
    "                            transformers[0]['parameter_grid'],\n",
    "                            transformers[1]['parameter_grid'],\n",
    "                            \n",
    "                            # approach's classifier\n",
    "                            classifier['parameter_grid'],\n",
    "                            [extra_parameters],\n",
    "\n",
    "                            # base classifier\n",
    "                            [{f\"{classifier['id']}__{approach['prefix']}classifier\": [base_classifier['model']]}],\n",
    "                            base_classifier_parameter_grids,\n",
    "                        ))\n",
    "\n",
    "                        # parameter grid\n",
    "                        for parameter_grid_combination in parameter_grid_combinations:\n",
    "                            parameter_grid.append(dict(collections.ChainMap(*parameter_grid_combination)))                    \n",
    "\n",
    "                    else: # if not parameter grid search\n",
    "                        # base classifier's parameter grid\n",
    "                        base_classifier_parameter_grid = { \n",
    "                            f\"{classifier['id']}__{approach['prefix']}{key}\": \n",
    "                                value for key, value in base_classifier['parameters'].items()\n",
    "                        }\n",
    "\n",
    "                        # parameter grid\n",
    "                        parameter_grid.append({\n",
    "                            # transformers\n",
    "                            **transformers[0]['parameters'],\n",
    "                            **transformers[1]['parameters'],\n",
    "\n",
    "                            # approach's classifier\n",
    "                            **classifier['parameters'],\n",
    "                            **extra_parameters,\n",
    "\n",
    "                            # base classifier\n",
    "                            f\"{classifier['id']}__{approach['prefix']}classifier\": [base_classifier['model']],\n",
    "                            **base_classifier_parameter_grid,\n",
    "                        })\n",
    "\n",
    "                    print(f\"{base_classifier['name']}\", end=': ')\n",
    "\n",
    "                    # grid search\n",
    "                    gs = GridSearchCV(\n",
    "                        pipeline, \n",
    "                        parameter_grid,\n",
    "                        scoring='jaccard_samples', \n",
    "                        n_jobs=-1 if base_classifier['supports_parallel'] else 1, # if supported from base classifier\n",
    "                        pre_dispatch='2*n_jobs',\n",
    "                        cv=cv,\n",
    "                        verbose=verbose, \n",
    "                        error_score=error\n",
    "                    )\n",
    "                    \n",
    "                    # time start\n",
    "                    t1 = time()\n",
    "\n",
    "                    # train\n",
    "                    gs.fit(ds['X_train'], ds['y_train'])\n",
    "\n",
    "                    # time finish\n",
    "                    t2 = time()\n",
    "                    \n",
    "                    # time difference in minutes\n",
    "                    td = t2 - t1\n",
    "\n",
    "                    # predict\n",
    "                    predicted = gs.predict(ds['X_test'])\n",
    "\n",
    "                    # model's results\n",
    "                    model = {\n",
    "                        'id': f\"{path}{approach['id']}_{classifier['id']}_{base_classifier['id']}\",\n",
    "                        'name': base_classifier['name'],\n",
    "                        'model': gs,\n",
    "                        'metrics': {\n",
    "                            'accuracy': metrics.jaccard_score(ds['y_test'], predicted, average='samples'),\n",
    "                            'subset_accuracy': metrics.accuracy_score(ds['y_test'], predicted),\n",
    "                            'precision': metrics.precision_score(ds['y_test'], predicted, average='samples'),\n",
    "                            'recall': metrics.recall_score(ds['y_test'], predicted, average='samples'),\n",
    "                            'f1': metrics.f1_score(ds['y_test'], predicted, average='samples')\n",
    "                        },\n",
    "                        'warnings': [str(w) for w in ws],\n",
    "                    }\n",
    "\n",
    "                    td_str = f\"{td:.2f}s\" if td < 100 else f\"{td/60:.2f}m\" if td < 10000 else f\"{td/3600:.2f}h\" \n",
    "                    print(f\"{model['metrics']['accuracy']:.2f}\", f\"[{td_str}]\", f\"({len(model['warnings'])})\")\n",
    "\n",
    "                    # save model to file\n",
    "                    dump([model, gs], f\"{path}{approach['id']}_{classifier['id']}_{base_classifier['id']}.gz\", compress=9)\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training on 100mentors → Testing on 100mentors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlb_hm = MultiLabelBinarizer(classes=np.array(sorted(HM_CLASSES)), sparse_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhm = mlb_hm.fit_transform(hmf['tags'].values)\n",
    "Xhm = hmf['text'].values\n",
    "\n",
    "print(Xhm.shape, yhm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tags:', ', '.join(mlb_hm.classes[np.where(yhm.toarray()[0] == 1)]))\n",
    "print('y:', yhm.toarray()[0])\n",
    "print('X:', Xhm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhm_train, Xhm_test, yhm_train, yhm_test = train_test_split(Xhm, yhm, test_size=1/5, random_state=3, shuffle=True)\n",
    "\n",
    "print('train dataset:', Xhm_train.shape, yhm_train.shape)\n",
    "print('test dataset:', Xhm_test.shape, yhm_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_ds = {\n",
    "    'X_train': Xhm_train,\n",
    "    'X_test': Xhm_test,\n",
    "    'y_train': yhm_train,\n",
    "    'y_test': yhm_test,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_models = execute(hm_ds, error='raise', path='./clfs/100mentors/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training on 100mentors & Wikipedia → Testing on 100mentors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=np.array(sorted(CLASSES)), sparse_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhm_acl = mlb.fit_transform(hmf['tags'].values)\n",
    "Xhm_acl = hmf['text'].values\n",
    "\n",
    "print(Xhm_acl.shape, yhm_acl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhm_acl_train, X_test, yhm_acl_train, y_test = train_test_split(Xhm_acl, yhm_acl, test_size=1/3, random_state=3, shuffle=True)\n",
    "\n",
    "print('test dataset:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ywk = mlb.fit_transform(wkf['tags'].values)\n",
    "Xwk = wkf['text'].values\n",
    "\n",
    "y_train = csr_matrix(np.concatenate([yhm_acl_train.toarray(), ywk.toarray()], axis=0))\n",
    "X_train = np.concatenate([Xhm_acl_train, Xwk], axis=0)\n",
    "\n",
    "print('train dataset:', X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk_ds = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test, \n",
    "    'y_train': y_train, \n",
    "    'y_test': y_test,\n",
    "}\n",
    "\n",
    "ex = [\n",
    "    'svm',\n",
    "    'mnb',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wk_models = execute(wk_ds, ex=ex, pgs=False, path='./clfs/wiki/')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "uni"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
